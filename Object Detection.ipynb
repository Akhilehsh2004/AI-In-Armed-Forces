{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce83b6-c069-43c6-9898-e314482ecedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Akhilesh Pant (AU FTCA: MCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd0ea26f-d14f-427c-8201-f74100245a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 648.7ms\n",
      "Speed: 155.2ms preprocess, 648.7ms inference, 79.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 192.2ms\n",
      "Speed: 3.3ms preprocess, 192.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 201.4ms\n",
      "Speed: 3.3ms preprocess, 201.4ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 222.8ms\n",
      "Speed: 3.4ms preprocess, 222.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 202.8ms\n",
      "Speed: 3.7ms preprocess, 202.8ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 211.1ms\n",
      "Speed: 2.7ms preprocess, 211.1ms inference, 6.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 258.2ms\n",
      "Speed: 4.4ms preprocess, 258.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 262.1ms\n",
      "Speed: 3.5ms preprocess, 262.1ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 293.9ms\n",
      "Speed: 3.1ms preprocess, 293.9ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 256.4ms\n",
      "Speed: 2.8ms preprocess, 256.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 269.3ms\n",
      "Speed: 4.7ms preprocess, 269.3ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 279.4ms\n",
      "Speed: 3.0ms preprocess, 279.4ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 252.8ms\n",
      "Speed: 3.4ms preprocess, 252.8ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 261.2ms\n",
      "Speed: 4.0ms preprocess, 261.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 279.0ms\n",
      "Speed: 2.2ms preprocess, 279.0ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 244.4ms\n",
      "Speed: 2.8ms preprocess, 244.4ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 288.0ms\n",
      "Speed: 2.6ms preprocess, 288.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 263.3ms\n",
      "Speed: 2.4ms preprocess, 263.3ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 278.7ms\n",
      "Speed: 2.7ms preprocess, 278.7ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 293.9ms\n",
      "Speed: 3.4ms preprocess, 293.9ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 301.6ms\n",
      "Speed: 5.3ms preprocess, 301.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 285.6ms\n",
      "Speed: 3.3ms preprocess, 285.6ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 254.8ms\n",
      "Speed: 2.8ms preprocess, 254.8ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 293.4ms\n",
      "Speed: 3.1ms preprocess, 293.4ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 298.5ms\n",
      "Speed: 5.8ms preprocess, 298.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 280.8ms\n",
      "Speed: 3.3ms preprocess, 280.8ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 305.4ms\n",
      "Speed: 3.9ms preprocess, 305.4ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 300.4ms\n",
      "Speed: 3.4ms preprocess, 300.4ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 319.2ms\n",
      "Speed: 5.0ms preprocess, 319.2ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 302.4ms\n",
      "Speed: 3.1ms preprocess, 302.4ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 299.2ms\n",
      "Speed: 4.2ms preprocess, 299.2ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 277.3ms\n",
      "Speed: 3.5ms preprocess, 277.3ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 294.4ms\n",
      "Speed: 3.1ms preprocess, 294.4ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 298.8ms\n",
      "Speed: 4.7ms preprocess, 298.8ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 310.3ms\n",
      "Speed: 4.0ms preprocess, 310.3ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 293.1ms\n",
      "Speed: 4.6ms preprocess, 293.1ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 245.6ms\n",
      "Speed: 3.1ms preprocess, 245.6ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 267.2ms\n",
      "Speed: 3.5ms preprocess, 267.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 269.0ms\n",
      "Speed: 3.6ms preprocess, 269.0ms inference, 6.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 280.2ms\n",
      "Speed: 5.6ms preprocess, 280.2ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 294.0ms\n",
      "Speed: 4.5ms preprocess, 294.0ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 302.1ms\n",
      "Speed: 2.9ms preprocess, 302.1ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 279.6ms\n",
      "Speed: 2.3ms preprocess, 279.6ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 306.0ms\n",
      "Speed: 4.3ms preprocess, 306.0ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 296.7ms\n",
      "Speed: 3.0ms preprocess, 296.7ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 295.8ms\n",
      "Speed: 3.7ms preprocess, 295.8ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 286.9ms\n",
      "Speed: 2.3ms preprocess, 286.9ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 282.8ms\n",
      "Speed: 2.8ms preprocess, 282.8ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 283.3ms\n",
      "Speed: 2.3ms preprocess, 283.3ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 292.5ms\n",
      "Speed: 4.6ms preprocess, 292.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 295.2ms\n",
      "Speed: 4.0ms preprocess, 295.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 290.5ms\n",
      "Speed: 3.0ms preprocess, 290.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 266.3ms\n",
      "Speed: 3.7ms preprocess, 266.3ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 273.1ms\n",
      "Speed: 3.1ms preprocess, 273.1ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 316.9ms\n",
      "Speed: 5.4ms preprocess, 316.9ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 296.8ms\n",
      "Speed: 5.1ms preprocess, 296.8ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 294.2ms\n",
      "Speed: 3.8ms preprocess, 294.2ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 289.8ms\n",
      "Speed: 3.1ms preprocess, 289.8ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 283.9ms\n",
      "Speed: 2.6ms preprocess, 283.9ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 296.6ms\n",
      "Speed: 3.8ms preprocess, 296.6ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 297.1ms\n",
      "Speed: 4.1ms preprocess, 297.1ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 283.0ms\n",
      "Speed: 2.8ms preprocess, 283.0ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 286.3ms\n",
      "Speed: 2.6ms preprocess, 286.3ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 307.3ms\n",
      "Speed: 2.7ms preprocess, 307.3ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 311.2ms\n",
      "Speed: 4.9ms preprocess, 311.2ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 197.6ms\n",
      "Speed: 3.1ms preprocess, 197.6ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 298.0ms\n",
      "Speed: 3.5ms preprocess, 298.0ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 324.0ms\n",
      "Speed: 2.5ms preprocess, 324.0ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 299.8ms\n",
      "Speed: 2.4ms preprocess, 299.8ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 301.9ms\n",
      "Speed: 5.6ms preprocess, 301.9ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 248.4ms\n",
      "Speed: 3.6ms preprocess, 248.4ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def detect_gaze(landmarks):\n",
    "    left_eye = landmarks[0:2]\n",
    "    right_eye = landmarks[2:4]\n",
    "    left_center = np.mean(left_eye, axis=0).astype(\"int\")\n",
    "    right_center = np.mean(right_eye, axis=0).astype(\"int\")\n",
    "\n",
    "    if left_center[0] < right_center[0]:\n",
    "        return \"Looking Right\"\n",
    "    elif left_center[0] > right_center[0]:\n",
    "        return \"Looking Left\"\n",
    "    else:\n",
    "        return \"Looking Straight\"\n",
    "\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh()\n",
    "    model = YOLO(\"yolov8n.pt\")  # Ensure this model file is available\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Object Detection\n",
    "        results = model(frame)\n",
    "        if results and results[0].boxes is not None:\n",
    "            for box in results[0].boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                conf = box.conf[0]\n",
    "                cls = int(box.cls[0])\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f\"{model.names[cls]} {conf:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "        # Gaze Detection\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                landmarks = []\n",
    "                for idx in [33, 133, 362, 263]:  # Eye landmarks\n",
    "                    x = int(face_landmarks.landmark[idx].x * frame.shape[1])\n",
    "                    y = int(face_landmarks.landmark[idx].y * frame.shape[0])\n",
    "                    landmarks.append((x, y))\n",
    "                    cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "                gaze_direction = detect_gaze(landmarks)\n",
    "                cv2.putText(frame, gaze_direction, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Defense AI - Object & Gaze Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69e8b1-06ad-4de2-bff1-e1cf8facd476",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe35a1-2ce5-47ca-bd3d-b597e99b1f51",
   "metadata": {},
   "source": [
    "Here's an explanation of the code **line by line**, covering the concepts behind each part:\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Imports**\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "```\n",
    "\n",
    "- **`cv2`**: OpenCV library used for real-time computer vision tasks like video capturing and image processing.  \n",
    "- **`numpy`**: Used for numerical operations, especially for array and mathematical operations.  \n",
    "- **`mediapipe`**: A framework by Google for building multimodal machine learning pipelines (e.g., face, hand, and body detection).  \n",
    "- **`YOLO`**: Imports the YOLO model (You Only Look Once) for real-time object detection.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Function: Gaze Detection**\n",
    "\n",
    "```python\n",
    "def detect_gaze(landmarks):\n",
    "    left_eye = landmarks[0:2]\n",
    "    right_eye = landmarks[2:4]\n",
    "```\n",
    "- The function **`detect_gaze`** takes a list of 4 eye landmarks.  \n",
    "- **`left_eye`**: Contains the first two landmarks (assumed as the left eye).  \n",
    "- **`right_eye`**: Contains the last two landmarks (assumed as the right eye).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    left_center = np.mean(left_eye, axis=0).astype(\"int\")\n",
    "    right_center = np.mean(right_eye, axis=0).astype(\"int\")\n",
    "```\n",
    "- **`np.mean()`**: Calculates the mean (average) position of the eye landmarks to get the **center of the eyes**.  \n",
    "- **`astype(\"int\")`**: Converts the result into integers for drawing and comparison.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    if left_center[0] < right_center[0]:\n",
    "        return \"Looking Right\"\n",
    "    elif left_center[0] > right_center[0]:\n",
    "        return \"Looking Left\"\n",
    "    else:\n",
    "        return \"Looking Straight\"\n",
    "```\n",
    "- Compares the **horizontal (X-axis)** positions of both eyes.  \n",
    "- If the left eye is more to the left than the right eye, it's **\"Looking Right\"** and vice versa.  \n",
    "- If the X-coordinates are similar, it's **\"Looking Straight\"**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Main Function for Detection**\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "```\n",
    "- Initializes the webcam using **`cv2.VideoCapture(0)`**, where `0` refers to the default camera.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”µ **Face Mesh Initialization**\n",
    "\n",
    "```python\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh()\n",
    "```\n",
    "- **`mp.solutions.face_mesh`**: Loads the **MediaPipe Face Mesh** solution.  \n",
    "- **`FaceMesh()`**: Initializes the face mesh model for detecting facial landmarks.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”µ **YOLO Model Initialization**\n",
    "\n",
    "```python\n",
    "    model = YOLO(\"yolov8n.pt\") \n",
    "```\n",
    "- Loads the **YOLOv8n (nano) model** for object detection.  \n",
    "- Assumes the model weights file (`yolov8n.pt`) is available.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Main Loop for Live Detection**\n",
    "\n",
    "```python\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "```\n",
    "- Continuously captures frames from the webcam.  \n",
    "- **`ret`**: Boolean indicating if the frame was successfully captured.  \n",
    "- **`frame`**: The actual image frame.  \n",
    "- **`break`**: Exits the loop if the frame isn't captured.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”µ **Object Detection with YOLO**\n",
    "\n",
    "```python\n",
    "        results = model(frame)\n",
    "```\n",
    "- Runs the YOLO model on the current **`frame`** to detect objects.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        if results and results[0].boxes is not None:\n",
    "            for box in results[0].boxes:\n",
    "```\n",
    "- Checks if any detection **`results`** exist and if bounding **`boxes`** are present.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "```\n",
    "- Extracts the **bounding box coordinates** (`x1`, `y1` for the top-left and `x2`, `y2` for the bottom-right) and converts them to integers.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "                conf = box.conf[0]\n",
    "                cls = int(box.cls[0])\n",
    "```\n",
    "- **`conf`**: Confidence score of the detected object.  \n",
    "- **`cls`**: Detected object's class index.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f\"{model.names[cls]} {conf:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "```\n",
    "- Draws a **blue rectangle** around the detected object.  \n",
    "- Displays the **class label** and **confidence score** above the rectangle.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”µ **Gaze Detection Using MediaPipe**\n",
    "\n",
    "```python\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "```\n",
    "- Converts the **BGR frame to RGB** because MediaPipe requires RGB format.  \n",
    "- Processes the frame to detect face landmarks.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "```\n",
    "- Checks if **multiple face landmarks** are detected.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "                landmarks = []\n",
    "                for idx in [33, 133, 362, 263]:\n",
    "```\n",
    "- Selects specific **landmark indices** for the eyes. These indices are predefined for eye regions in MediaPipe.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "                    x = int(face_landmarks.landmark[idx].x * frame.shape[1])\n",
    "                    y = int(face_landmarks.landmark[idx].y * frame.shape[0])\n",
    "                    landmarks.append((x, y))\n",
    "```\n",
    "- Converts normalized landmark coordinates to **pixel coordinates** by multiplying them with the frame dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "                    cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "```\n",
    "- Draws **green circles** at the detected eye landmarks for visualization.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "                gaze_direction = detect_gaze(landmarks)\n",
    "                cv2.putText(frame, gaze_direction, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "```\n",
    "- Calls the **`detect_gaze`** function to determine the gaze direction.  \n",
    "- Displays the gaze direction on the screen.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”µ **Display the Frame**\n",
    "\n",
    "```python\n",
    "        cv2.imshow(\"Defense AI - Object & Gaze Detection\", frame)\n",
    "```\n",
    "- Shows the final frame with object detection and gaze direction in a window.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”µ **Exit Condition**\n",
    "\n",
    "```python\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "```\n",
    "- Waits for a key press. If the key is **'q'**, it exits the loop.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Release Resources**\n",
    "\n",
    "```python\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "```\n",
    "- Releases the webcam resource.  \n",
    "- Closes all OpenCV windows.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Script Execution**\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "- Ensures that the **`main()`** function runs **only if the script is executed** directly.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¥ **Key Concepts Covered**:\n",
    "- **Real-Time Object Detection** with YOLO.  \n",
    "- **Facial Landmark Detection** with MediaPipe.  \n",
    "- **Gaze Detection Logic** by comparing eye landmark positions.  \n",
    "- **Drawing and Annotating** images with OpenCV.  \n",
    "- **Efficient Loop Control** for real-time video processing.\n",
    "\n",
    "---\n",
    "\n",
    "This code efficiently combines **object detection** and **gaze direction detection** for potential defense or surveillance applications. Let me know if you'd like to modify or enhance any part of it! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
